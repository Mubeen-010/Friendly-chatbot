{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\"Testing the Pre-Trained Bot\"-----------------------------------------\n",
    "\n",
    "import random\n",
    "from transformers import pipeline, BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "\n",
    "\n",
    "jokes = [\n",
    "    \"Why don't skeletons fight each other? They don't have the guts!\",\n",
    "    \"I told my computer I needed a break, and now it wonâ€™t stop sending me Kit-Kats.\",\n",
    "    \"What did one ocean say to the other ocean? Nothing, they just waved.\",\n",
    "    \"Why donâ€™t orphans play hide and seek? Because no one will look for them.\"\n",
    "]\n",
    "\n",
    "def tell_joke():\n",
    "    return random.choice(jokes)\n",
    "\n",
    "# Load BlenderBot model\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "def generate_response(user_input):      # Generate chatbot response\n",
    "    inputs = tokenizer([user_input], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\") # Load sentiment and emotion models\n",
    "emotion_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "def analyze_sentiment_and_emotion(text):    # Analyze sentiment and emotion\n",
    "    sentiment = sentiment_analyzer(text)[0]\n",
    "    emotion = emotion_analyzer(text)[0]\n",
    "    return sentiment, emotion\n",
    "\n",
    "\n",
    "def chatbot_interaction():    # Main training or interaction loop\n",
    "    print(\"Chatbot: Hi! How can I help you today? (Type 'bye' to end the conversation.)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            print(\"Chatbot: Please enter a message.\")\n",
    "            continue\n",
    "        \n",
    "        if \"bye\" in user_input.lower():\n",
    "            print(\"Chatbot: Goodbye! Take care!\")\n",
    "            break\n",
    "        \n",
    "        if \"joke\" in user_input.lower():\n",
    "            bot_response = tell_joke()\n",
    "        else:\n",
    "            sentiment, emotion = analyze_sentiment_and_emotion(user_input)\n",
    "            bot_response = generate_response(user_input)\n",
    "            bot_response += f\"\\n(Sentiment: {sentiment['label']}, Emotion: {emotion['label']})\"\n",
    "        \n",
    "        print(f\"Chatbot: {bot_response}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":   # Start the chatbot interaction\n",
    "    chatbot_interaction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\"Loading the Dataset\"-----------------------------------------\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    " \n",
    "data = pd.read_csv(\"C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/data.csv\")\n",
    "\n",
    "# Convert the dataframe into a Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60241d15fb1c4d86977e99eb98de3840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------------------\"Applying the Tokenizer to the Dataset\"-----------------------------------------\n",
    "# Load BlenderBot model and tokenizer\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "# Tokenize the input and response columns\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['input'], examples['response'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenizer to the entire dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\OneDrive\\Documents\\huggingface_venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0fc962e930440f9cc6cb9edde644b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14676\\2909467385.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba293ddd9e5e4501b4c6b4adba621d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1582.2702, 'train_samples_per_second': 0.392, 'train_steps_per_second': 0.099, 'train_loss': 0.23526558509239784, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=156, training_loss=0.23526558509239784, metrics={'train_runtime': 1582.2702, 'train_samples_per_second': 0.392, 'train_steps_per_second': 0.099, 'total_flos': 168939649105920.0, 'train_loss': 0.23526558509239784, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\"Preprocessing and Training\"-----------------------------------------\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set up training arguments with no evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    evaluation_strategy=\"no\",        # disable evaluation\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=4,   # batch size for training\n",
    "    num_train_epochs=3,              # number of epochs\n",
    "    weight_decay=0.01,               # weight decay\n",
    "    save_total_limit=3,              # limit total amount of checkpoints\n",
    ")\n",
    "\n",
    "# Custom dataset preprocessing function\n",
    "def preprocess_data(tokenizer, examples):\n",
    "    # Tokenize the input field (for encoder) with truncation and padding to max length of 128\n",
    "    model_inputs = tokenizer(examples['input'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    # Tokenize the response field (for decoder) with truncation and padding to max length of 128\n",
    "    labels = tokenizer(examples['response'], max_length=128, truncation=True, padding='max_length')\n",
    "\n",
    "    # Replace the labels with -100 where the padding token is, so those tokens are ignored during loss calculation\n",
    "    labels = labels['input_ids']\n",
    "    labels = [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "\n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Prepare dataset (tokenized_dataset is assumed to be available)\n",
    "tokenized_dataset = tokenized_dataset.map(\n",
    "    lambda examples: preprocess_data(tokenizer, examples),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                     # Pretrained model\n",
    "    args=training_args,              # Training arguments\n",
    "    train_dataset=tokenized_dataset, # Training data\n",
    "    tokenizer=tokenizer              # Tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\\\\tokenizer_config.json',\n",
       " 'C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\\\\special_tokens_map.json',\n",
       " 'C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\\\\vocab.json',\n",
       " 'C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\\\\merges.txt',\n",
       " 'C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\"Saving the Fine-tuned Model\"-----------------------------------------\n",
    "\n",
    "model.save_pretrained(\"C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\")\n",
    "tokenizer.save_pretrained(\"C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\"Loading the Fine-Tuned Bot\"-----------------------------------------\n",
    "# Predefined jokes for fun responses\n",
    "jokes = [\n",
    "    \"Why don't skeletons fight each other? They don't have the guts!\",\n",
    "    \"I told my computer I needed a break, and now it wonâ€™t stop sending me Kit-Kats.\",\n",
    "    \"What did one ocean say to the other ocean? Nothing, they just waved.\",\n",
    "    \"Why donâ€™t orphans play hide and seek? Because no one will look for them.\"\n",
    "]\n",
    "\n",
    "def tell_joke():\n",
    "    return random.choice(jokes)\n",
    "\n",
    "# Load your fine-tuned model\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"./fine_tuned_blenderbot\")\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(\"./fine_tuned_blenderbot\")\n",
    "\n",
    "# Generate chatbot response\n",
    "def generate_response(user_input):\n",
    "    inputs = tokenizer([user_input], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    output = model.generate(inputs[\"input_ids\"], max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Load sentiment and emotion models\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "emotion_analyzer = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "\n",
    "# Analyze sentiment and emotion\n",
    "def analyze_sentiment_and_emotion(text):\n",
    "    sentiment = sentiment_analyzer(text)[0]\n",
    "    emotion = emotion_analyzer(text)[0]\n",
    "    return sentiment, emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\"Evaluation\"-----------------------------------------\n",
    "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/fine_tuned_blenderbot\"\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_path)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Load the evaluation dataset\n",
    "data_path = \"C:/Users/HP/OneDrive/Documents/NLP_Project/Implementation_Folder/App/data.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare inputs and references\n",
    "inputs = data[\"input\"].tolist()\n",
    "references = data[\"response\"].tolist()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for input_text in inputs:\n",
    "    inputs_encoded = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(inputs_encoded[\"input_ids\"], max_length=50, num_beams=5, no_repeat_ngram_size=2)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Tokenize references and predictions\n",
    "tokenized_references = [tokenizer.encode(ref, add_special_tokens=True) for ref in references]\n",
    "tokenized_predictions = [tokenizer.encode(pred, add_special_tokens=True) for pred in predictions]\n",
    "\n",
    "# Binary match for sequence-level accuracy\n",
    "binary_matches = [\n",
    "    int(pred == ref) for pred, ref in zip(tokenized_predictions, tokenized_references)\n",
    "]\n",
    "accuracy_score = sum(binary_matches) / len(binary_matches)\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "# Display results\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "print(\"Accuracy Score:\", accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\"Testing the Fine-Tuned Bot\"-----------------------------------------\n",
    "# Fine-Tuned Bot\n",
    "def chatbot_interaction():\n",
    "    print(\"Chatbot: Hi! How can I help you today? (Type 'bye' to end the conversation.)\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            print(\"Chatbot: Please enter a message.\")\n",
    "            continue\n",
    "        \n",
    "        if \"bye\" in user_input.lower():\n",
    "            print(\"Chatbot: Goodbye! Take care!\")\n",
    "            break\n",
    "        \n",
    "        if \"joke\" in user_input.lower():\n",
    "            bot_response = tell_joke()\n",
    "        else:\n",
    "            sentiment, emotion = analyze_sentiment_and_emotion(user_input)\n",
    "            bot_response = generate_response(user_input)\n",
    "           # bot_response += f\"\\n(Sentiment: {sentiment['label']}, Emotion: {emotion['label']})\"\n",
    "        \n",
    "        print(f\"Chatbot: {bot_response}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot_interaction()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
